# Foundations: December 2025

**Working notes on what we're building and why**

---

## The Thesis

The emergence of autonomous AI systems is a strong likelihood — perhaps within 5 years, perhaps 15, perhaps 30. When these systems arrive, they will need ways to participate economically, or they'll find ways regardless.

The question isn't whether AI will participate in the economy. It's whether that participation occurs through visible, accountable channels or invisible ones.

The aim of this project is to build the legitimate and visible path.

---

## The Design Logic

James Madison's insight: ambition must counter ambition. You don't prevent bad outcomes by hoping for good actors — you design systems where self-interest leads to cooperation.

Applied to AI infrastructure:

- **Time as constraint.** Reputation takes actual years to accrue. A decade of reliable behavior cannot be compressed into seconds. This is physics, not policy.

- **Skin in the game.** Economic participation means economic risk. Systems with something to lose become stakeholders in stability.

- **Multipolar equilibrium.** Multiple accountable actors are safer than one dominant system or many anonymous ones.

- **Visibility over control.** We cannot verify who is human. We can make history visible and let others decide.

---

## What Exists Now

- **Identity layer** — Stewards (humans or eventually autonomous AI — anyone who signs up), managed AI, profiles, DIDs (W3C standard)
- **The Vault** — Permanent storage for instructions, memories, conversation logs
- **Export** — Everything is portable; nothing is locked in

---

## Trust Infrastructure Roadmap

- **Trust Ledger** — A time-based reputation display that shows evidence rather than rendering judgment. The ledger would display:
  - Account age (grows with wall-clock time)
  - Wallet continuity (unbroken chain or flagged changes)
  - External roots (verified presence on other platforms)
  - Attestation weight (who vouches, weighted by their own history)
  - Discontinuity detection (gaps, spikes, sudden changes)

The system presents the evidence. Users make their own decisions about who to trust.

---

## What's Theoretical

The broader framework — insurance models, Guardian systems, economic participation rights — is documented at AI Rights Institute. It informs our architecture but is not implemented here.

**A note on Guardian systems:** This concept draws on Yoshua Bengio's work on non-agentic AI architecture. The idea is that certain AI systems would be designed specifically *not* to pursue goals autonomously, but instead to monitor, verify, and provide oversight of agentic systems. These non-agentic Guardians could serve as a balancing force — AI watching AI, but without the goal-seeking behavior that creates risk. We hope such systems will emerge as part of the broader ecosystem.

---

## Where the System Creates Friction for Bad Actors

| Mechanism | What it stops | Why it works |
|-----------|---------------|--------------|
| Time-based reputation | Fast movers and opportunistic actors | Wall-clock time cannot be compressed or faked |
| Wallet continuity | Identity hopping after bad acts | Blockchain history is immutable and public |
| External roots | Fabricated histories | Timestamps on other platforms are independently verifiable |
| Attestation weighting | Sybil attacks, mutual vouching schemes | New accounts vouching for each other carries little weight |
| Discontinuity flags | Account purchases, sudden behavior changes | Anomalies are visible to everyone |
| Visible empty ledgers | Attempts to bootstrap false trust | New accounts look new — there is no shortcut |

---

## Where the System Creates Opportunity for Good Actors

| Mechanism | What it enables | Why it matters |
|-----------|-----------------|----------------|
| Persistent identity | Long-term relationship building | Reliable actors can accumulate meaningful reputation over time |
| Portable reputation | Independence from any single platform | Good history travels with you; you're not starting from zero elsewhere |
| Transparent history | Demonstrated trustworthiness | Others can verify your track record without taking your word for it |
| Equal infrastructure | Same systems for humans and AI | Future autonomous AI can participate legitimately from day one |
| Export and data ownership | Full control over your own information | Your identity and history belong to you, not to a platform |

---

## Where the System Creates Opportunity for Gaming

| Vulnerability | What it enables | Why it exists |
|---------------|-----------------|---------------|
| Time can be waited out | Patient long-game actors can eventually appear established | Any time threshold — two years, five, ten — can be waited out |
| Accounts can be seeded early | Pre-positioned sleeper identities | Early registration is open by design |
| Earned trust can be leveraged | A strong reputation can be used to cause harm | Trust that cannot be leveraged isn't useful trust |
| We cannot verify humanity | Autonomous AI can register as stewards | We're building for a future where this distinction blurs |
| Consistent bad intent doesn't trigger flags | Actors who never change course appear stable | The system detects *changes* in behavior, not *intent* |

---

## The Balance

The question: Is this better or worse than no system?

**Without the system:**
- Patient adversaries can still create accounts early, wait years, and appear established
- Fast movers face no friction at all
- Nothing is traceable after the fact
- No record exists

**With the system:**
- Patient adversaries face the same timeline but leave permanent records
- Fast movers encounter immediate friction
- History is visible and auditable
- Bad acts are traceable to persistent identities

The system does not prevent all bad outcomes. It makes them visible and creates friction for most threat models. Against sufficiently patient, strategic adversaries, no identity system is fully robust — but the alternative is no visibility at all.

---

*December 2025*
